{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls training_set/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = glob.glob('train/cats/*')\n",
    "D = glob.glob('train/dogs/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read cats images\n",
    "CX = []\n",
    "CY = [0]*len(C)\n",
    "for i in C:\n",
    "    CX.append(np.array(Image.open(i).convert('L')))\n",
    "\n",
    "CX = np.array(CX).reshape(np.array(CX).shape[0],-1)\n",
    "CY = np.array(CY).reshape(-1,1)\n",
    "print(CX.shape)\n",
    "print(CY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read dogs image\n",
    "DX = []\n",
    "DY = [0]*len(D)\n",
    "for i in D:\n",
    "    DX.append(np.array(Image.open(i).convert('L')))\n",
    "\n",
    "DX = np.array(DX).reshape(np.array(DX).shape[0],-1)\n",
    "DY = np.array(DY).reshape(-1,1)\n",
    "DY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 1) (160, 1)\n"
     ]
    }
   ],
   "source": [
    "#join dogs and cats images\n",
    "X = np.concatenate((CX,DX))\n",
    "Y = np.concatenate((CY,DY))\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet():\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "    \n",
    "        # Values of the hidden layer\n",
    "    \n",
    "        X1 = np.maximum( X.dot(W1) + b1, 0 )\n",
    "        X2 = X1.dot(W2) + b2\n",
    "        \n",
    "        scores = X2\n",
    "        \n",
    "        if y is None:\n",
    "          return scores\n",
    "   \n",
    "        # Creating a output matrix from output vector\n",
    "        # X2 is the output\n",
    "        Y = np.zeros(X2.shape)\n",
    "        for i,row in enumerate(Y):\n",
    "            row[y[i]] = 1\n",
    "        \n",
    "        loss = 0.0\n",
    "        \n",
    "        #Exponentiating the X2 , then normalizing the exponentiated value\n",
    "        exp_X2 = pow(math.e, X2)\n",
    "        scores   = (exp_X2.T/np.sum(exp_X2, axis = 1)).T\n",
    "    \n",
    "        for i in range(N):\n",
    "            loss -= np.log( scores[i][y[i]] )\n",
    "        loss /= N\n",
    "        loss += 0.5*reg*np.sum(W1 * W1)  \n",
    "        loss += 0.5*reg*np.sum(W2 * W2) \n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        \n",
    "        #Back propagation\n",
    "        dX2 = scores - Y\n",
    "        dW2 = ((dX2.T).dot(X1)).T/N + reg*W2\n",
    "        db2 = (dX2.T).dot(np.ones(X1.shape[0]))/N\n",
    "        dX1 = dX2.dot(W2.T)\n",
    "        dW1 = (((dX1*(X1>0)).T).dot(X)).T/N + reg*W1\n",
    "        db1 = (((dX1*(X1>0)).T).dot(np.ones(X.shape[0]))).T/N\n",
    "      \n",
    "        grads['W2'] = dW2\n",
    "        grads['b2'] = db2\n",
    "        grads['W1'] = dW1\n",
    "        grads['b1'] = db1\n",
    "        \n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "              reg=1e-5, num_iters=100,batch_size=200, \n",
    "              verbose=False ,dropout_fraction = 0):\n",
    "        \n",
    "        Binom_variables = np.random.choice([0, 1], size=X.shape, p=[dropout_fraction,1-dropout_fraction])\n",
    "        X = (X*Binom_variables)/(1-dropout_fraction)\n",
    "        \n",
    "        \n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "        val_acc = 0\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        for it in xrange(num_iters):\n",
    "            batch_indicies = np.random.choice(num_train, batch_size, replace = False)\n",
    "            X_batch = X[batch_indicies]\n",
    "            y_batch = y[batch_indicies]\n",
    "            \n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "        \n",
    "            for variable in self.params:\n",
    "                self.params[variable] -= learning_rate*grads[variable]\n",
    "            \n",
    "            if verbose and it % 100 == 0:\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                print ('iteration %d / %d: loss %f, val_acc %f' % (it, num_iters, loss, val_acc))\n",
    "            \n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "        \n",
    "        return {\n",
    "            'loss_history': loss_history,\n",
    "            'train_acc_history': train_acc_history,\n",
    "            'val_acc_history': val_acc_history,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, X):\n",
    "        y_pred = None\n",
    "        X1 = np.maximum( X.dot(self.params['W1']) + self.params['b1'], 0 )\n",
    "        X2 = X1.dot(self.params['W2']) + self.params['b2']\n",
    "        exp_X2 = pow(math.e, X2)\n",
    "        scores   = (exp_X2.T/np.sum(exp_X2, axis = 1)).T\n",
    "    \n",
    "        y_pred = np.argmax(scores, axis = 1)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TwoLayerNet' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-d7d6039a8a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mCandD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCandD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TwoLayerNet' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "CandD = TwoLayerNet(100,2,0)\n",
    "CandD.train(X, Y, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
